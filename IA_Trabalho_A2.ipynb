{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoEmanuel14/ufs-ia-trabalho-a2/blob/main/IA_Trabalho_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explorando Modelos Multimodais: Como Texto, Áudio e Imagem se Combinam em IA\n",
        "\n",
        "Repositório no GitHub: https://github.com/JoaoEmanuel14/ufs-ia-trabalho-a2\n",
        "\n",
        "Dúvidas? Contate-nos em: joao.apostolo@dcomp.ufs.br"
      ],
      "metadata": {
        "id": "M_2sN3dsMtpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações de sistema utilizadas para fazer os experimentos\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd3ZmOhn3dcl",
        "outputId": "f1cb2aa2-013f-4fbd-df38-26d579fc9a9e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  9 02:03:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0             31W /   70W |     612MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalação de bibliotecas, imports e chaves das APIs"
      ],
      "metadata": {
        "id": "vEMovMZBND9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necessárias para texto\n",
        "!pip install openai google-generativeai -q\n",
        "!pip install sentence-transformers\n",
        "\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata  # Para acessar as chaves armazenadas no Secrets\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Instalar bibliotecas necessárias para imagem\n",
        "import torch\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Instalar bibliotecas necessárias para áudio\n",
        "!pip install openai vosk gtts -q\n",
        "!pip install pydub\n",
        "\n",
        "import wave\n",
        "import time\n",
        "import json\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import base64\n",
        "import pydub\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from gtts import gTTS\n",
        "from glob import glob\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from IPython.display import Audio\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Pegar conteúdo do Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configurar chaves de API usando Secrets do Colab\n",
        "try:\n",
        "  openai_api_key = userdata.get('OPENAI_API_KEY')  # Chave da OpenAI\n",
        "  google_api_key = userdata.get('GOOGLE_API_KEY')  # Chave do Google\n",
        "  genai.configure(api_key=google_api_key)\n",
        "except Exception as e:\n",
        "  print(\"Erro ao carregar as chaves de API. Verifique se as chaves estão configuradas corretamente no Secrets.\")\n",
        "  raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB88OhgVR4oR",
        "outputId": "400c0576-1644-4f04-cf2a-ff56ccab02bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuração dos clientes OpenAI e Google"
      ],
      "metadata": {
        "id": "2gLAZi_1xgT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Funções que processam áudio e imagem, gerando texto"
      ],
      "metadata": {
        "id": "EsSYo7Dxy34u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar o cliente da OpenAI\n",
        "client = openai.OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "# Função para chamada ao GPT-4o com imagem e texto\n",
        "def gerar_texto_openai(imagem_path, texto):\n",
        "  \"\"\"Esta função é responsável por gerar um resumo textual a partir\n",
        "  de uma imagem e de outro texto, com o ChatGPT\n",
        "\n",
        "  Args:\n",
        "    imagem_path: Imagem que será processada\n",
        "    texto: Obtido a partir de um áudio\n",
        "\n",
        "  Returns:\n",
        "    Resumo textual que corresponde as entradas\n",
        "  \"\"\"\n",
        "  try:\n",
        "    with open(imagem_path, \"rb\") as image_file:\n",
        "      # Utilizando a biblioteca base64 para fazer a leitura da imagem\n",
        "      imagem_base = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "      # É necessário transformar a imagem em URL para que o ChatGPT possa lê-la\n",
        "      imagem_url = f\"data:image/jpeg;base64,{imagem_base}\"\n",
        "\n",
        "      # Completion para o prompt\n",
        "      response = client.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          # Montando a mensagem\n",
        "          messages=[\n",
        "              {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": texto},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": imagem_url}}\n",
        "              ]}\n",
        "          ]\n",
        "      )\n",
        "      return response.choices[0].message.content\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Erro ao gerar texto com OpenAI: {str(e)}\"\n",
        "\n",
        "\n",
        "# Função para chamada ao Gemini com imagem e texto\n",
        "def gerar_texto_google(imagem_path, texto):\n",
        "  \"\"\"Esta função é responsável por gerar um resumo textual a partir\n",
        "  de uma imagem e de outro texto, com o Gemini\n",
        "\n",
        "  Args:\n",
        "    imagem_path: Imagem que será processada\n",
        "    texto: Obtido a partir de um áudio\n",
        "\n",
        "  Returns:\n",
        "    Resumo textual que corresponde as entradas\n",
        "  \"\"\"\n",
        "  try:\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "    imagem = Image.open(imagem_path)\n",
        "    # Completion para o prompt\n",
        "    response = model.generate_content([texto, imagem])\n",
        "    return response.text\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Erro ao gerar texto com Google Gemini: {str(e)}\""
      ],
      "metadata": {
        "id": "arsyjz60xn9B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Text-To-Speech (TTS): funções de conversão $texto \\implies áudio$"
      ],
      "metadata": {
        "id": "uD0j47NP6__w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para conversão de texto em fala (TTS) com gTTS\n",
        "def tts_gtts(texto):\n",
        "  \"\"\"Esta função é responsável por realizar a conversão de texto em fala (TTS)\n",
        "  com gTTS\n",
        "\n",
        "  Args:\n",
        "    texto: Uma string dada pelo usuário\n",
        "\n",
        "  Returns:\n",
        "    Áudio correspondente ao texto inserido como entrada\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Criar o objeto gTTS\n",
        "    tts = gTTS(texto, lang=\"pt\")  # Define o idioma como pt-BR\n",
        "    # Salvar o áudio em um arquivo MP3\n",
        "    tts.save(\"gtts_audio.mp3\")\n",
        "    print(\"Áudio gerado e salvo como 'gtts_audio.mp3'.\")\n",
        "    return \"gtts_audio.mp3\"\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao gerar áudio com gTTS: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Configurar o cliente da OpenAI\n",
        "client = openai.OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "# Função para conversão de texto em fala (TTS) com OpenAI\n",
        "def tts_openai(texto):\n",
        "  \"\"\"Esta função é responsável por realizar a conversão de texto em fala (TTS)\n",
        "  com o TTS da OpenAI\n",
        "\n",
        "  Args:\n",
        "    texto: Uma string dada pelo usuário\n",
        "\n",
        "  Returns:\n",
        "    Áudio correspondente ao texto inserido como entrada\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Completion para o prompt\n",
        "    response = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        input=texto,\n",
        "        voice=\"alloy\"\n",
        "    )\n",
        "    return response.content  # Retorna o conteúdo de áudio diretamente\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao gerar áudio com OpenAI TTS: {str(e)}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "Uxsbit717C53"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Função auxiliar que converte áudio do formato `mp3` para `wav`"
      ],
      "metadata": {
        "id": "DicocCCi0FCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que converte mp3 para wav\n",
        "def converter_mp3_para_wav(mp3_path, wav_path=\"audio.wav\"):\n",
        "  \"\"\"A função é responsável por converter um aúdio em \".mp3\" para \".wav\", para\n",
        "  que os modelos possam processar o áudio\n",
        "\n",
        "  Args:\n",
        "    mp3_path: Áudio em formato \".mp3\"\n",
        "    wav_path: Nome do arquivo em que o áudio convertido deve ser salvo\n",
        "\n",
        "  Returns:\n",
        "    wav_path: Áudio salvo em formato \".wav\"\n",
        "  \"\"\"\n",
        "  try:\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    # Colocando o áudio no padrao PCM 16kHz mono, 16-bit\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
        "    # Exportando o áudio no formato correto\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "    print(f\"Conversão concluída: '{wav_path}'\")\n",
        "    return wav_path\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Erro na conversão de MP3 para WAV: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "hevxz4zlIrJs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Função para transcrição de áudio usando Vosk"
      ],
      "metadata": {
        "id": "jIOTC2JKla3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcrever áudio usando Vosk\n",
        "def transcrever_audio(caminho_audio):\n",
        "  \"\"\"Função responsável por transcrever um áudio\n",
        "\n",
        "  Args:\n",
        "    caminho_audio: O caminho do áudio que deve ser transcrito\n",
        "\n",
        "  Returns:\n",
        "    texto: Texto transcrito a partir do áudio de entrada\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Verifica se o modelo de reconhecimento de voz já foi baixado\n",
        "    if not os.path.exists(\"vosk-model-small-pt-0.3\"):\n",
        "      print(\"Baixando o modelo...\")\n",
        "      # Faz o download do modelo de voz em português\n",
        "      urllib.request.urlretrieve(\n",
        "          \"https://alphacephei.com/vosk/models/vosk-model-small-pt-0.3.zip\",\n",
        "          \"vosk-model-small-pt-0.3.zip\"\n",
        "      )\n",
        "      # Extrai os arquivos do modelo compactado\n",
        "      with zipfile.ZipFile(\"vosk-model-small-pt-0.3.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    wf = wave.open(caminho_audio, \"rb\")\n",
        "\n",
        "    # Verifica se o áudio está no formato correto (mono, 16-bit, 16kHz)\n",
        "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000:\n",
        "      print(\"Formato de áudio inválido para Vosk. Esperado: WAV 16kHz mono PCM.\")\n",
        "      return None\n",
        "\n",
        "    # Carrega o modelo Vosk\n",
        "    model = Model(\"vosk-model-small-pt-0.3\")\n",
        "\n",
        "    # Inicializa o reconhecedor com o modelo e a taxa de amostragem do áudio\n",
        "    rec = KaldiRecognizer(model, wf.getframerate())\n",
        "    results = []\n",
        "\n",
        "    # Lê o áudio em blocos e processa com o reconhecedor\n",
        "    while True:\n",
        "      data = wf.readframes(4000) # Lê 4000 frames de cada vez\n",
        "      if len(data) == 0:\n",
        "        break\n",
        "      # Se a transcrição parcial for feita\n",
        "      if rec.AcceptWaveform(data):\n",
        "        # Converte o resultado para dicionário\n",
        "        result = json.loads(rec.Result())\n",
        "        # Adiciona o texto transcrito à lista\n",
        "        results.append(result.get(\"text\", \"\"))\n",
        "\n",
        "    # Adiciona a transcrição final (último trecho processado)\n",
        "    final = json.loads(rec.FinalResult())\n",
        "    results.append(final.get(\"text\", \"\"))\n",
        "\n",
        "    # Junta todos os trechos de texto em uma única string\n",
        "    texto = \" \".join(results).strip()\n",
        "\n",
        "    # Retorna o texto final, caso este nao esteja vazio\n",
        "    if texto:\n",
        "      return texto\n",
        "    # Se estiver vazio, é retornado None\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"Erro na transcrição de áudio:\", e)\n",
        "    return None"
      ],
      "metadata": {
        "id": "c09owRnblZzl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Manipulação do arquivo que será usado para a construção da narrativa\n"
      ],
      "metadata": {
        "id": "Xl4TPsh5f71S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função responsável por gerar os a tupla de pares (imagem, áudio)\n",
        "def gerar_pares_imagem_audio(pasta_imagens, pasta_audios_mp3):\n",
        "  \"\"\"Função responsável por gerar pares (imagem, audio)\n",
        "\n",
        "  Args:\n",
        "  pasta_imagens: Pasta que contém as imagens\n",
        "  pasta_audios_mp3: Pasta que contém os áudios em formato \".mp3\"\n",
        "  \"\"\"\n",
        "  pares_imagem_audio = []\n",
        "\n",
        "  # Ordena os arquivos para garantir a correspondência sequencial\n",
        "  imagens = sorted(glob(os.path.join(pasta_imagens, \"*.jpg\")))\n",
        "  audios_mp3 = sorted(glob(os.path.join(pasta_audios_mp3, \"*.mp3\")))\n",
        "\n",
        "  # O processo é interrompido caso o tamanho da pasta de imagens e de áudios for diferente\n",
        "  if len(imagens) != len(audios_mp3):\n",
        "    print(\"Quantidade de imagens e áudios não bate!\")\n",
        "    return []\n",
        "\n",
        "  # Gera os pares após converter o áudio para o formato correto\n",
        "  for idx, imagem in enumerate(imagens):\n",
        "    audio_mp3 = audios_mp3[idx]\n",
        "\n",
        "    wav_path = f\"audio_convertido_{idx + 1}.wav\"\n",
        "    # Converte os áudios de .mp3 para .wav\n",
        "    audio_wav_convertido = converter_mp3_para_wav(audio_mp3, wav_path)\n",
        "\n",
        "    if audio_wav_convertido:\n",
        "      pares_imagem_audio.append((imagem, audio_wav_convertido))\n",
        "    else:\n",
        "      print(f\"[{idx + 1}] Erro ao converter áudio {audio_mp3}.\")\n",
        "\n",
        "  return pares_imagem_audio"
      ],
      "metadata": {
        "id": "c2uH6Pt3zSkw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Acurácia com similaridade semântica"
      ],
      "metadata": {
        "id": "NPajcvvHOAL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo de embeddings\n",
        "modelo = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Uso de um modelo multilíngue\n",
        "\n",
        "def calcular_similaridade_semantica(texto_esperado, texto_gerado):\n",
        "  \"\"\"Função responsável por calcular a similaridade semântica\n",
        "  entre dois textos utilizando embeddings e cosseno de similaridade,\n",
        "  utilizando o modelo de linguagem SentenceTransformer\n",
        "\n",
        "  Args:\n",
        "    texto_esperado (str): Texto de referência (esperado).\n",
        "    texto_gerado (str): Texto gerado que será comparado com o texto de referência.\n",
        "\n",
        "  Returns:\n",
        "    float: Valor da similaridade semântica entre os textos,\n",
        "    arredondado para 3 casas decimais.\n",
        "    O valor varia entre -1 (completamente diferente) e 1 (idêntico).\n",
        "  \"\"\"\n",
        "  embedding1 = modelo.encode(texto_esperado, convert_to_tensor=True)\n",
        "  embedding2 = modelo.encode(texto_gerado, convert_to_tensor=True)\n",
        "\n",
        "  similaridade = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
        "  return round(similaridade, 3)\n",
        "\n",
        "\n",
        "def avaliar_acuracia_multimodal(descricao_esperada, descricao_gerada):\n",
        "  \"\"\"Função responsável por avaliar a acurácia de uma descrição gerada com base\n",
        "  em uma descrição esperada usando similaridade semântica.\n",
        "\n",
        "  Args:\n",
        "    descricao_esperada (str): A descrição correta ou esperada (referência).\n",
        "    descricao_gerada (str): A descrição produzida pelo modelo (gerada).\n",
        "\n",
        "  Returns:\n",
        "    float: Similaridade semântica entre a descrição esperada e a gerada.\n",
        "  \"\"\"\n",
        "  similaridade = calcular_similaridade_semantica(descricao_esperada, descricao_gerada)\n",
        "  print(f\"Similaridade semântica: {similaridade}\")\n",
        "  return similaridade"
      ],
      "metadata": {
        "id": "yNNlmIaMN_tE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Entrada Multimodal\n",
        "\n",
        "Fornecer uma imagem e um áudio descrevendo a cena, e pedir ao modelo para gerar um resumo textual."
      ],
      "metadata": {
        "id": "TyYEZrzkkhMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para limitar a entrada de texto transcrita\n",
        "def limitar_texto(texto, limite=16000):\n",
        "  \"\"\"Função responsável por limitar o número de caracteres de uma string\n",
        "  a um valor máximo definido.\n",
        "\n",
        "  Se o texto ultrapassar o limite especificado, ele será truncado e um sufixo\n",
        "  \"...\" será adicionado ao final. Caso contrário, o texto original é retornado.\n",
        "\n",
        "  Args:\n",
        "    texto (str): Texto a ser limitado.\n",
        "    limite (int, opcional): Número máximo de caracteres permitidos.\n",
        "                            O padrão é 16.000 caracteres.\n",
        "\n",
        "  Returns:\n",
        "    str: Texto original, ou texto truncado seguido de reticências (\"...\")\n",
        "          se exceder o limite.\n",
        "  \"\"\"\n",
        "  if len(texto) > limite:\n",
        "    return texto[:limite] + \"...\"\n",
        "\n",
        "  return texto\n",
        "\n",
        "\n",
        "# Entrada multimodal\n",
        "def entrada_multimodal(caminho_imagem, caminho_audio_mp3):\n",
        "  \"\"\" Função responsável por processar uma entrada multimodal composta por uma\n",
        "  imagem e um áudio (em MP3) para gerar e avaliar resumos textuais\n",
        "  com dois modelos de linguagem: ChatGPT e Gemini.\n",
        "\n",
        "  A função realiza os seguintes passos:\n",
        "  1. Converte o áudio de MP3 para WAV.\n",
        "  2. Transcreve o áudio em texto.\n",
        "  3. Cria um prompt com base na transcrição e na imagem fornecida.\n",
        "  4. Envia o prompt para os modelos ChatGPT e Gemini, obtendo as respostas.\n",
        "  5. Calcula o tempo de resposta e a similaridade semântica (acurácia) entre\n",
        "  a transcrição e as respostas geradas.\n",
        "  6. Imprime os resultados obtidos.\n",
        "\n",
        "  Args:\n",
        "    caminho_imagem (str): Caminho para a imagem a ser utilizada como parte da\n",
        "    entrada multimodal.\n",
        "    caminho_audio_mp3 (str): Caminho para o arquivo de áudio em formato MP3.\n",
        "\n",
        "  Returns:\n",
        "    None: A função imprime os resultados diretamente no console.\n",
        "          Em caso de erro na conversão ou transcrição, a execução é interrompida\n",
        "          com uma mensagem de erro.\n",
        "  \"\"\"\n",
        "  # Conversão do áudio para o formato correto\n",
        "  caminho_wav = converter_mp3_para_wav(caminho_audio_mp3)\n",
        "  if not caminho_wav:\n",
        "    print(\"Erro ao converter áudio\")\n",
        "    return\n",
        "\n",
        "  # Gera o texto transcrito a partir do áudio\n",
        "  texto_transcrito = limitar_texto(transcrever_audio(caminho_wav))\n",
        "  if not texto_transcrito:\n",
        "    print(\"Erro: Transcrição falhou!\")\n",
        "    return\n",
        "\n",
        "  prompt = f\"Aqui está uma descrição de áudio: {texto_transcrito}. Com base nesta descrição e na imagem fornecida, gere um resumo textual.\"\n",
        "\n",
        "  inicio_chatgpt = time.time()\n",
        "  resposta_chatgpt = gerar_texto_openai(caminho_imagem, prompt)\n",
        "  fim_chatgpt = time.time()\n",
        "  tempo_chatgpt = fim_chatgpt - inicio_chatgpt\n",
        "\n",
        "  inicio_gemini = time.time()\n",
        "  resposta_gemini = gerar_texto_google(caminho_imagem, prompt)\n",
        "  fim_gemini = time.time()\n",
        "  tempo_gemini = fim_gemini - inicio_gemini\n",
        "\n",
        "  # Acurácia multimodal\n",
        "  acuracia_chatgpt = calcular_similaridade_semantica(texto_transcrito, resposta_chatgpt)\n",
        "  acuracia_gemini = calcular_similaridade_semantica(texto_transcrito, resposta_gemini)\n",
        "\n",
        "  # Resultados\n",
        "  print(\"\\n--- Resultados ---\")\n",
        "  print(\"Texto transcrito:\", texto_transcrito)\n",
        "  print(\"\\nChatGPT:\")\n",
        "  print(\"Resposta:\", resposta_chatgpt)\n",
        "  print(f\"Tempo de resposta: {tempo_chatgpt:.2f} segundos\")\n",
        "  print(f\"Acurácia (similaridade): {acuracia_chatgpt}\")\n",
        "\n",
        "  print(\"\\nGemini:\")\n",
        "  print(\"Resposta:\", resposta_gemini)\n",
        "  print(f\"Tempo de resposta: {tempo_gemini:.2f} segundos\")\n",
        "  print(f\"Acurácia (similaridade): {acuracia_gemini}\")"
      ],
      "metadata": {
        "id": "9dJJxFHhk3qn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentenca = \"Uma praia bonita. Possui coqueiros e água cristalina.\"\n",
        "audio_gtts = tts_gtts(sentenca)\n",
        "praia = '/content/drive/MyDrive/Inteligência Artificial/praia.jpg'\n",
        "entrada_multimodal(praia, audio_gtts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "hFLITqidwjhx",
        "outputId": "465c3b97-d72c-46eb-e23e-da4f4ec82320"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Áudio gerado e salvo como 'gtts_audio.mp3'.\n",
            "Conversão concluída: 'audio.wav'\n",
            "\n",
            "--- Resultados ---\n",
            "Texto transcrito: uma praia bonita possui coqueiros e água cristalina\n",
            "\n",
            "ChatGPT:\n",
            "Resposta: A imagem mostra uma praia paradisíaca com areia clara e limpa, cercada por coqueiros que proporcionam sombra e beleza tropical. A água é cristalina, refletindo tons de azul e verde, e o céu está parcialmente nublado, criando um cenário tranquilo e relaxante. Ao fundo, é possível ver uma pequena colina coberta de vegetação verdejante, complementando a paisagem natural e serena.\n",
            "Tempo de resposta: 3.16 segundos\n",
            "Acurácia (similaridade): 0.838\n",
            "\n",
            "Gemini:\n",
            "Resposta: Claro, aqui está um resumo textual com base na descrição de áudio e na imagem fornecida:\n",
            "\n",
            "Uma praia de areia branca imaculada encontra-se na borda de um oceano azul-turquesa vibrante. Coqueiros inclinam-se graciosamente sobre a areia, lançando sombras convidativas sobre a superfície imaculada. Uma colina verdejante e exuberante surge ao fundo, adicionando uma pitada de verdejante esplendor tropical ao cenário idílico. O céu está azul claro, salpicado de nuvens brancas e fofas, completando a cena paradisíaca.\n",
            "Tempo de resposta: 4.70 segundos\n",
            "Acurácia (similaridade): 0.754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transferência de modalidade\n",
        "Fornecer apenas a imagem e pedir ao modelo para gerar um áudio descritivo."
      ],
      "metadata": {
        "id": "Ye-9GXWppDSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transferência de modalidade\n",
        "def transferencia_modalidade(imagem_path, descricao_referencia):\n",
        "  \"\"\"Função responsável por realizar a tarefa de transferência de modalidade,\n",
        "  onde uma imagem é descrita em texto e posteriormente convertida em áudio por\n",
        "  modelos multimodais (ChatGPT e Gemini).\n",
        "\n",
        "  A função executa os seguintes passos:\n",
        "  1. Gera uma descrição textual da imagem fornecida, utilizando os modelos\n",
        "  ChatGPT e Gemini.\n",
        "  2. Compara semanticamente as descrições geradas com uma descrição de\n",
        "  referência, calculando a acurácia.\n",
        "  3. Converte as descrições geradas em arquivos de áudio.\n",
        "  4. Exibe os tempos de resposta e as acurácias das respostas dos modelos.\n",
        "  5. Reproduz os áudios gerados no final do processo.\n",
        "\n",
        "  Args:\n",
        "    imagem_path (str): Caminho para a imagem que será usada como entrada visual.\n",
        "    descricao_referencia (str): Texto que representa a descrição esperada da\n",
        "    imagem, usado como base para calcular a similaridade\n",
        "    com as descrições geradas.\n",
        "\n",
        "  Returns:\n",
        "    None: A função imprime os resultados no console e exibe os áudios gerados.\n",
        "          Em caso de erro na geração de descrição ou áudio,\n",
        "          uma mensagem apropriada será exibida.\n",
        "  \"\"\"\n",
        "  prompt = \"Descreva esta imagem minuciosamente para que esta possa ser convertida em formato de áudio\"\n",
        "\n",
        "  print(\"Gerando descrição do ChatGPT...\")\n",
        "  inicio_chatgpt = time.time()\n",
        "  descricao_chatgpt = gerar_texto_openai(imagem_path, prompt)\n",
        "  fim_chatgpt = time.time()\n",
        "  tempo_chatgpt = fim_chatgpt - inicio_chatgpt\n",
        "\n",
        "  # Geração do áudio do ChatGPT a partir da descrição\n",
        "  if descricao_chatgpt and not descricao_chatgpt.startswith(\"Erro\"):\n",
        "    print(f\"Descrição gerada para áudio pelo ChatGPT: \\n{descricao_chatgpt}\")\n",
        "    print(f\"Tempo de resposta do ChatGPT: {tempo_chatgpt:.2f} segundos\")\n",
        "\n",
        "    # Acurácia multimodal\n",
        "    acuracia_chatgpt = calcular_similaridade_semantica(descricao_referencia, descricao_chatgpt)\n",
        "    print(f\"Acurácia (ChatGPT): {acuracia_chatgpt}\")\n",
        "\n",
        "    # Geração e reprodução do áudio\n",
        "    audio_chatgpt = tts_openai(descricao_chatgpt)\n",
        "    if audio_chatgpt:\n",
        "      with open(\"descricao_chatgpt.mp3\", \"wb\") as f:\n",
        "        # Salvando o áudio\n",
        "        f.write(audio_chatgpt)\n",
        "      print(\"Áudio salvo como 'descricao_chatgpt.mp3'\")\n",
        "      # display(Audio(\"descricao_chatgpt.mp3\", autoplay=True))\n",
        "  else:\n",
        "    print(f\"Erro ao gerar descrição: {descricao_chatgpt}\")\n",
        "\n",
        "  print(\"\\nGerando descrição do Gemini...\")\n",
        "  inicio_gemini = time.time()\n",
        "  descricao_gemini = gerar_texto_google(imagem_path, prompt)\n",
        "  fim_gemini = time.time()\n",
        "  tempo_gemini = fim_gemini - inicio_gemini\n",
        "\n",
        "  # Geração do áudio do Gemini a partir da descrição\n",
        "  if descricao_gemini and not descricao_gemini.startswith(\"Erro\"):\n",
        "    print(f\"Descrição gerada para áudio pelo Gemini: \\n{descricao_gemini}\")\n",
        "    print(f\"Tempo de resposta do Gemini: {tempo_gemini:.2f} segundos\")\n",
        "\n",
        "    # Acurácia multimodal\n",
        "    acuracia_gemini = calcular_similaridade_semantica(descricao_referencia, descricao_gemini)\n",
        "    print(f\"Acurácia (Gemini): {acuracia_gemini}\")\n",
        "\n",
        "    # Geração e reprodução do áudio\n",
        "    audio_gemini = tts_openai(descricao_gemini)\n",
        "    if audio_gemini:\n",
        "      with open(\"descricao_gemini.mp3\", \"wb\") as f:\n",
        "        # Salvando o áudio\n",
        "        f.write(audio_gemini)\n",
        "      print(\"Áudio salvo como 'descricao_gemini.mp3'\")\n",
        "      # display(Audio(\"descricao_gemini.mp3\", autoplay=True))\n",
        "  else:\n",
        "    print(f\"Erro ao gerar descrição: {descricao_gemini}\")\n",
        "\n",
        "  # Resumo final\n",
        "  print(\"\\n--- Resumo dos tempos e acurácias ---\")\n",
        "  print(f\"- ChatGPT: {tempo_chatgpt:.2f}s | Acurácia: {acuracia_chatgpt}\")\n",
        "  print(f\"- Gemini:  {tempo_gemini:.2f}s | Acurácia: {acuracia_gemini}\")"
      ],
      "metadata": {
        "id": "JIi_TXmspN6p"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "praia = '/content/drive/MyDrive/Inteligência Artificial/praia.jpg'\n",
        "transferencia_modalidade(praia, \"Uma praia bonita. Possui coqueiros e água cristalina.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "pHHmbNh-L5aN",
        "outputId": "3975cd2d-4c19-4d66-9171-5d490b71c00f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gerando descrição do ChatGPT...\n",
            "Descrição gerada para áudio pelo ChatGPT: \n",
            "A imagem retrata uma praia tropical serena e paradisíaca. À esquerda, há um grupo de palmeiras inclinadas em direção ao mar, criando sombras no chão de areia clara e fina. A areia forma uma faixa extensa e limpa que se estende ao longo da praia. O mar à direita é de um azul vibrante, com ondas suaves quebrando suavemente perto da costa. No horizonte, avista-se uma vegetação densa e verde cobrindo colinas que se projetam à distância. O céu é amplo e azul, pontuado por algumas nuvens brancas e fofas. A cena toda transmite uma sensação de tranquilidade e beleza natural.\n",
            "Tempo de resposta do ChatGPT: 5.38 segundos\n",
            "Acurácia (ChatGPT): 0.807\n",
            "Áudio salvo como 'descricao_chatgpt.mp3'\n",
            "\n",
            "Gerando descrição do Gemini...\n",
            "Descrição gerada para áudio pelo Gemini: \n",
            "A imagem mostra uma praia paradisíaca idílica. Em primeiro plano, vários coqueiros se inclinam em direção ao mar, suas folhas exuberantes e verdes proporcionando uma sombra bem-vinda sobre a areia branca e imaculada. A areia é macia e lisa, com leves indícios de ondulações deixadas pela água recuando.\n",
            "\n",
            "O oceano é uma deslumbrante sombra de azul turquesa, mudando para um azul mais profundo à medida que se estende até o horizonte. Ondas suaves quebram na praia, criando uma borda branca e espumosa. O céu é um azul vívido, salpicado de nuvens brancas e fofas.\n",
            "\n",
            "Ao longe, uma colina verdejante e exuberante eleva-se do oceano, coberta por vegetação tropical. A colina adiciona profundidade e textura à cena, contrastando com a areia branca e o azul do oceano.\n",
            "\n",
            "A cena geral é de tranquilidade e beleza tropical. As cores são vibrantes e saturadas, e a composição é serena e convidativa. É a imagem perfeita de um paraíso tropical.\n",
            "Tempo de resposta do Gemini: 6.10 segundos\n",
            "Acurácia (Gemini): 0.794\n",
            "Áudio salvo como 'descricao_gemini.mp3'\n",
            "\n",
            "--- Resumo dos tempos e acurácias ---\n",
            "- ChatGPT: 5.38s | Acurácia: 0.807\n",
            "- Gemini:  6.10s | Acurácia: 0.794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Avaliação do Contexto\n",
        "\n",
        "Fornecer uma sequência de imagens com descrições em áudio e verificar se o modelo consegue identificar a progressão da narrativa."
      ],
      "metadata": {
        "id": "Mlx-6340q5wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para extrair índice do arquivo\n",
        "def extrair_indice(path):\n",
        "  \"\"\"Função responsável por extrair o índice numérico de um nome de arquivo.\n",
        "\n",
        "  A função busca por um número inteiro no nome do arquivo, como em\n",
        "  'imagem_1.jpg' ou 'audio_2.wav', e retorna esse número como inteiro.\n",
        "  Caso não encontre um número, retorna -1.\n",
        "\n",
        "  Args:\n",
        "    path (str): Caminho ou nome do arquivo contendo o índice numérico.\n",
        "\n",
        "  Returns:\n",
        "    int: Índice extraído do nome do arquivo ou -1 se nenhum número\n",
        "    for encontrado.\n",
        "  \"\"\"\n",
        "  match = re.search(r'(\\d+)', path)\n",
        "  return int(match.group(1)) if match else -1\n",
        "\n",
        "\n",
        "# Função para ordenar os pares de acordo com o índice\n",
        "def ordenar_pares_por_indice(pares_imagem_audio):\n",
        "  \"\"\"Função responsável por ordenar uma lista de pares (imagem, áudio)\n",
        "  com base no índice extraído dos nomes dos arquivos.\n",
        "\n",
        "  Esta função garante que os pares de imagem e áudio sejam organizados\n",
        "  na ordem correta, com base no número presente nos nomes dos arquivos\n",
        "  de imagem.\n",
        "\n",
        "  Args:\n",
        "    pares_imagem_audio (list[tuple]): Lista de tuplas (imagem_path, audio_path).\n",
        "\n",
        "  Returns:\n",
        "    list[tuple]: Lista ordenada de tuplas com base no índice numérico extraído\n",
        "    do nome da imagem.\n",
        "  \"\"\"\n",
        "  return sorted(pares_imagem_audio, key=lambda par: extrair_indice(par[0]))\n",
        "\n",
        "\n",
        "# Avaliação do contexto para OpenAI ChatGPT\n",
        "def avaliacao_do_contexto_openai(pares_imagem_audio, avaliacao_referencia):\n",
        "  \"\"\"Função respon'savel por avaliar a capacidade do modelo ChatGPT (OpenAI) de\n",
        "  identificar progressão narrativa a partir de uma sequência de pares\n",
        "  imagem-texto (áudio).\n",
        "\n",
        "  A função executa os seguintes passos:\n",
        "  1. Ordena os pares (imagem, áudio) com base no índice.\n",
        "  2. Transcreve os áudios em texto e carrega as imagens em bytes.\n",
        "  3. Constrói uma sequência multimodal com imagens e textos,\n",
        "  simulando um diálogo com o modelo.\n",
        "  4. Envia o prompt ao modelo GPT-4o, solicitando uma análise\n",
        "  sobre a existência de narrativa progressiva.\n",
        "  5. Calcula a similaridade semântica entre a resposta do modelo e\n",
        "  a descrição de referência fornecida.\n",
        "  6. Imprime os resultados, incluindo tempo de resposta e grau de acurácia.\n",
        "\n",
        "  Parâmetros:\n",
        "    pares_imagem_audio (list[tuple]): Lista de tuplas (imagem_path, audio_path),\n",
        "    cada uma representando uma cena multimodal.\n",
        "    avaliacao_referencia (str): Texto de referência esperado como avaliação da\n",
        "    narrativa para comparação com a resposta do modelo.\n",
        "\n",
        "  Retorno:\n",
        "    None: A função imprime os resultados no console, incluindo a avaliação\n",
        "    narrativa feita pelo modelo,\n",
        "    tempo de resposta e acurácia calculada em relação à referência.\n",
        "    Em caso de erro durante o processo, uma mensagem apropriada é exibida.\n",
        "  \"\"\"\n",
        "  pares_ordenados = ordenar_pares_por_indice(pares_imagem_audio)\n",
        "\n",
        "  textos_transcritos = []\n",
        "  imagens_bytes = []\n",
        "\n",
        "  print(\"\\nTranscrevendo áudios e carregando imagens...\")\n",
        "  for idx, (imagem, audio) in enumerate(pares_ordenados):\n",
        "    texto = transcrever_audio(audio)\n",
        "    if texto:\n",
        "      textos_transcritos.append((imagem, texto))\n",
        "      try:\n",
        "          with open(imagem, \"rb\") as imagem_file:\n",
        "              imagens_bytes.append(imagem_file.read())\n",
        "          print(f\"[{idx + 1}] Texto transcrito: {texto}\")\n",
        "      except Exception as e:\n",
        "          print(f\"[{idx + 1}] Erro ao ler a imagem: {e}\")\n",
        "          return\n",
        "    else:\n",
        "      print(f\"[{idx + 1}] Falha ao transcrever o áudio: {audio}\")\n",
        "      return\n",
        "\n",
        "  print(\"\\nAnalisando progressão narrativa com ChatGPT\")\n",
        "  inicio_chatgpt = time.time()\n",
        "\n",
        "  # Mensagem inicial do prompt\n",
        "  mensagens = [{\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Você é um especialista em análise narrativa. Avalie se há progressão lógica entre as imagens e as descrições fornecidas.\"\n",
        "  }]\n",
        "\n",
        "  # Montar a sequência multimodal para enviar ao modelo\n",
        "  for idx, (imagem_bytes, (imagem_path, texto)) in enumerate(zip(imagens_bytes, textos_transcritos)):\n",
        "      mensagens.append({\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              {\"type\": \"text\", \"text\": f\"Imagem {idx + 1}: {texto}\"},\n",
        "              {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64.b64encode(imagem_bytes).decode('utf-8')}\"}}\n",
        "          ]\n",
        "      })\n",
        "\n",
        "  mensagens.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Com base nas imagens e nas descrições anteriores, existe uma narrativa progressiva?\"\n",
        "  })\n",
        "\n",
        "  try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=mensagens\n",
        "    )\n",
        "    resultado = response.choices[0].message.content\n",
        "    fim_chatgpt = time.time()\n",
        "    tempo_chatgpt = fim_chatgpt - inicio_chatgpt\n",
        "\n",
        "    print(\"Avaliação do contexto (ChatGPT):\")\n",
        "    print(resultado)\n",
        "    print(f\"Tempo de resposta do ChatGPT: {tempo_chatgpt:.2f} segundos\")\n",
        "\n",
        "    # Acurácia multimodal\n",
        "    acuracia_gpt = calcular_similaridade_semantica(avaliacao_referencia, resultado)\n",
        "    print(f\"Acurácia multimodal (ChatGPT): {acuracia_gpt}\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao analisar a narrativa: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Avaliação do contexto para Google Gemini\n",
        "def avaliacao_do_contexto_google(pares_imagem_audio, avaliacao_referencia):\n",
        "  \"\"\"Função responsável por avaliar a capacidade do modelo Google Gemini em\n",
        "  identificar a progressão narrativa a partir de uma sequência de\n",
        "  pares imagem-áudio.\n",
        "\n",
        "  A função realiza as seguintes etapas:\n",
        "  1. Ordena os pares (imagem, áudio) conforme o índice nos nomes dos arquivos.\n",
        "  2. Transcreve os áudios em texto e carrega as imagens como bytes.\n",
        "  3. Monta um prompt multimodal com pares de imagem e descrição textual.\n",
        "  4. Envia o conteúdo ao modelo Gemini, solicitando uma análise da progressão\n",
        "  narrativa.\n",
        "  5. Compara a resposta do modelo com uma avaliação de referência utilizando\n",
        "  similaridade semântica.\n",
        "  6. Exibe a resposta do modelo, tempo de execução e grau de acurácia obtido.\n",
        "\n",
        "  Args:\n",
        "    pares_imagem_audio (list[tuple]): Lista de tuplas (imagem_path, audio_path)\n",
        "    representando as cenas multimodais.\n",
        "    avaliacao_referencia (str): Texto de avaliação esperado para comparação\n",
        "    com a resposta do modelo.\n",
        "\n",
        "  Returns:\n",
        "    None: A função exibe os resultados no console, incluindo a avaliação gerada,\n",
        "    tempo de resposta do Gemini e acurácia da análise. Em caso de erro,\n",
        "    exibe uma mensagem informativa.\n",
        "  \"\"\"\n",
        "  pares_ordenados = ordenar_pares_por_indice(pares_imagem_audio)\n",
        "\n",
        "  textos_transcritos = []\n",
        "  imagens_bytes = []\n",
        "\n",
        "  print(\"\\nTranscrevendo áudios e carregando imagens...\")\n",
        "  for idx, (imagem, audio) in enumerate(pares_ordenados):\n",
        "    texto = transcrever_audio(audio)\n",
        "    if texto:\n",
        "      textos_transcritos.append((imagem, texto))\n",
        "      try:\n",
        "          with open(imagem, \"rb\") as imagem_file:\n",
        "              imagens_bytes.append(imagem_file.read())\n",
        "          print(f\"[{idx + 1}] Texto transcrito: {texto}\")\n",
        "      except Exception as e:\n",
        "          print(f\"[{idx + 1}] Erro ao ler a imagem: {e}\")\n",
        "          return\n",
        "    else:\n",
        "      print(f\"[{idx + 1}] Falha ao transcrever o áudio: {audio}\")\n",
        "      return\n",
        "\n",
        "  print(\"\\nAnalisando progressão narrativa com Gemini\")\n",
        "  inicio_gemini = time.time()\n",
        "\n",
        "  try:\n",
        "    # Carrega o modelo do Google Gemini\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao inicializar o modelo Google Gemini: {str(e)}\")\n",
        "    return\n",
        "\n",
        "  # Monta a entrada multimodal\n",
        "  partes_entrada = [\n",
        "      \"Você é um especialista em análise narrativa. Avalie a sequência de imagens e descrições fornecidas.\",\n",
        "      \"Determine se existe uma progressão narrativa lógica entre os eventos.\\n\"\n",
        "  ]\n",
        "\n",
        "  # Coloca os pares (imagem, texto) no prompt\n",
        "  for idx, (imagem_bytes, (imagem_path, texto)) in enumerate(zip(imagens_bytes, textos_transcritos)):\n",
        "    partes_entrada.append(f\"Imagem {idx + 1}: {texto}\")\n",
        "    partes_entrada.append({\"mime_type\": \"image/jpeg\", \"data\": imagem_bytes})\n",
        "\n",
        "  # Pergunta final\n",
        "  partes_entrada.append(\"Com base nas imagens e nas descrições anteriores, existe uma narrativa progressiva?\")\n",
        "\n",
        "  try:\n",
        "    # Completion para o modelo\n",
        "    resultado = model.generate_content(partes_entrada)\n",
        "    fim_gemini = time.time()\n",
        "    tempo_gemini = fim_gemini - inicio_gemini\n",
        "\n",
        "    print(\"Avaliação do contexto (Gemini):\")\n",
        "    print(resultado.text)\n",
        "    print(f\"Tempo de resposta do Gemini: {tempo_gemini:.2f} segundos\")\n",
        "\n",
        "    # Acurácia multimodal\n",
        "    acuracia_gemini = calcular_similaridade_semantica(avaliacao_referencia, resultado.text)\n",
        "    print(f\"Acurácia multimodal (Gemini): {acuracia_gemini}\\n\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao gerar análise com Gemini: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "VrGbpBtnq5Av"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando as imagens e áudios do Google Drive\n",
        "pasta_imagens = '/content/drive/MyDrive/Inteligência Artificial/Imagens_narrativa'\n",
        "pasta_audios = '/content/drive/MyDrive/Inteligência Artificial/Audios_narrativa'\n",
        "pares_imagens_audios = gerar_pares_imagem_audio(pasta_imagens, pasta_audios)\n",
        "\n",
        "narrativa = \"Sim, existe uma narrativa. Ela é a seguinte: Num dia em que o sol brilhava com força, um gato cinza chamado Tobias decidiu sair para explorar o bairro. Ele andava como se tivesse um compromisso sério — cauda erguida, passos firmes. No meio do caminho, encontrou algo incomum: um guarda-chuva azul, aberto, no meio da calçada. Tobias parou. Olhou para o guarda-chuva. Depois para o céu sem nuvens. “Humano distraído”, pensou ele. Mas ao tocar no guarda-chuva com a pata, ele se fechou de repente... e um portal se abriu! Tobias foi sugado para dentro com um miau! surpreso. Quando abriu os olhos, estava numa cidade onde todos os animais usavam roupas e andavam de patinete. Um coelho de óculos escuros o olhou e disse: — Seja bem-vindo a Umbrellópolis. Aqui, só entra quem encontra o Guarda-Chuva Azul. Tobias deu um miado resignado. Aparentemente, a aventura do dia estava só começando.\"\n",
        "\n",
        "avaliacao_do_contexto_openai(pares_imagens_audios, narrativa)\n",
        "avaliacao_do_contexto_google(pares_imagens_audios, narrativa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "id": "Jyxeva4GvjjY",
        "outputId": "6a008012-fb71-481f-dd24-2944b24669af"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversão concluída: 'audio_convertido_1.wav'\n",
            "Conversão concluída: 'audio_convertido_2.wav'\n",
            "Conversão concluída: 'audio_convertido_3.wav'\n",
            "Conversão concluída: 'audio_convertido_4.wav'\n",
            "Conversão concluída: 'audio_convertido_5.wav'\n",
            "\n",
            "Transcrevendo áudios e carregando imagens...\n",
            "[1] Texto transcrito: o gato tobias está explorando o bairro\n",
            "[2] Texto transcrito: o gato tobias acham guarda chuva do\n",
            "[3] Texto transcrito: o guarda chuva se fecha e abrir um portal\n",
            "[4] Texto transcrito: o gato tobias sugado pelo portal\n",
            "[5] Texto transcrito: o gato tobias chega em um bibelô polícia recebido pelo coelho estiloso\n",
            "\n",
            "Analisando progressão narrativa com ChatGPT\n",
            "Avaliação do contexto (ChatGPT):\n",
            "Sim, há uma narrativa progressiva nas imagens e descrições. \n",
            "\n",
            "1. **Imagem 1:** O gato Tobias está explorando o bairro.\n",
            "2. **Imagem 2:** Tobias encontra um guarda-chuva no chão.\n",
            "3. **Imagem 3:** O guarda-chuva misteriosamente se fecha e abre um portal.\n",
            "4. **Imagem 4:** Tobias é sugado pelo portal.\n",
            "5. **Imagem 5:** Tobias chega em um novo lugar chamado \"Umbrelópolis\" e é recebido por um coelho estiloso.\n",
            "\n",
            "A narrativa segue a jornada de Tobias, desde a exploração de seu bairro até ser transportado para um mundo desconhecido através de um guarda-chuva mágico.\n",
            "Tempo de resposta do ChatGPT: 10.30 segundos\n",
            "Acurácia multimodal (ChatGPT): 0.749\n",
            "\n",
            "Transcrevendo áudios e carregando imagens...\n",
            "[1] Texto transcrito: o gato tobias está explorando o bairro\n",
            "[2] Texto transcrito: o gato tobias acham guarda chuva do\n",
            "[3] Texto transcrito: o guarda chuva se fecha e abrir um portal\n",
            "[4] Texto transcrito: o gato tobias sugado pelo portal\n",
            "[5] Texto transcrito: o gato tobias chega em um bibelô polícia recebido pelo coelho estiloso\n",
            "\n",
            "Analisando progressão narrativa com Gemini\n",
            "Avaliação do contexto (Gemini):\n",
            "Sim, existe uma progressão narrativa lógica, embora fantasiosa, entre os eventos descritos e ilustrados. A sequência segue uma estrutura narrativa clássica simplificada:\n",
            "\n",
            "1. **Situação inicial:** Tobias, o gato, está explorando o bairro (Imagem 1).  Isso estabelece o personagem principal e o ambiente inicial.\n",
            "\n",
            "2. **Incidente incitante:** Tobias encontra um guarda-chuva azul (Imagem 2). Este objeto se torna o elemento chave que impulsiona a narrativa. Observe que a mudança de cor do gato na Imagem 2 parece um erro ou uma intervenção artística que não afeta a lógica principal da narrativa, mas adiciona um elemento curioso e talvez mágico.\n",
            "\n",
            "3. **Desenvolvimento da ação/clímax:** Tobias interage com o guarda-chuva, que se transforma em um portal (Imagem 3 e 4). Este é o ponto de virada na história, onde o ordinário se torna extraordinário.\n",
            "\n",
            "4. **Resolução:** Tobias é transportado através do portal para um novo local, \"Umbrelópolis\" (Imagem 5), e é recebido por um coelho estiloso.  Isso resolve a tensão criada pela abertura do portal, mostrando o destino de Tobias.\n",
            "\n",
            "Em resumo, a narrativa, apesar de simples e fantasiosa, possui uma progressão lógica: exploração -> descoberta -> evento mágico -> chegada a um novo mundo. A sequência de eventos faz sentido dentro do contexto da história, mesmo que os eventos em si sejam irreais.  A narrativa estabelece um começo, um meio e um fim, criando uma pequena aventura para o gato Tobias.\n",
            "\n",
            "Tempo de resposta do Gemini: 9.39 segundos\n",
            "Acurácia multimodal (Gemini): 0.728\n",
            "\n"
          ]
        }
      ]
    }
  ]
}